############################################################################################################################
# DATE: 24 July 2017
# AUTHOR: Alessandro Negrini
# DESCRIPTION: THIS IS NOT A CODE THAT CAN BE COMPILED, IT IS ONLY A COLLECTION OF SPARKS COMMANDS LAUNCHED ON SPARK-SHELL. 
EVEN OUTPUTS HAVE BEEN REPORTED
############################################################################################################################
# open the shell
> spark-shell

SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/lib/flume-ng/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel).
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.6.0
      /_/

Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_60)
Type in expressions to have them evaluated.
Type :help for more information.
17/07/24 22:47:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/24 22:47:26 WARN util.Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 192.168.245.146 instead (on interface eth1)
17/07/24 22:47:26 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/24 22:47:40 WARN shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
Spark context available as sc (master = local[*], app id = local-1500922049117).
SQL context available as sqlContext.

# load dataset (that is stored in a cluster) into an RDD
scala> val rdd_origin = sc.textFile("/user/training/ScrutiniFI.csv");
rdd_origin: org.apache.spark.rdd.RDD[String] = /user/training/ScrutiniFI.csv MapPartitionsRDD[1] at textFile at <console>:27

scala> var num_rows = rdd_origin.count()
num_rows: Long = 8000

scala> val rdd_origin_split = rdd_origin.map(line => line.split(';'))
rdd_origin_split: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[2] at map at <console>:29

scala> rdd_origin_split.take(5)
res0: Array[Array[String]] = Array(Array(DESCREGIONE, DESCPROVINCIA, DESCCOMUNE, ELETTORI, ELETTORI_M, VOTANTI, VOTANTI_M, NUMVOTISI, NUMVOTINO, NUMVOTIBIANCHI, NUMVOTINONVALIDI, NUMVOTICONTESTATI), Array(ABRUZZO, "CHIETI                        ", "ALTINO                                                                                              ", 2288, 1101, 1496, 775, 533, 953, 2, 8, 0), Array(ABRUZZO, "CHIETI                        ", "ARCHI                                                                                               ", 1785, 861, 1241, 632, 442, 782, 3, 14, 0), Array(ABRUZZO, "CHIETI                        ", "ARI                                                                                                 ", 831, 402, 617, 328, 241, 366, 6, 4, 0), Array(ABRUZZO...

scala> val rdd_origin_split_trim = rdd_origin_split.map(arr => arr.map(_.trim))
rdd_origin_split_trim: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[3] at map at <console>:31

scala> rdd_origin_split_trim.take(5)
res1: Array[Array[String]] = Array(Array(DESCREGIONE, DESCPROVINCIA, DESCCOMUNE, ELETTORI, ELETTORI_M, VOTANTI, VOTANTI_M, NUMVOTISI, NUMVOTINO, NUMVOTIBIANCHI, NUMVOTINONVALIDI, NUMVOTICONTESTATI), Array(ABRUZZO, CHIETI, ALTINO, 2288, 1101, 1496, 775, 533, 953, 2, 8, 0), Array(ABRUZZO, CHIETI, ARCHI, 1785, 861, 1241, 632, 442, 782, 3, 14, 0), Array(ABRUZZO, CHIETI, ARI, 831, 402, 617, 328, 241, 366, 6, 4, 0), Array(ABRUZZO, CHIETI, ARIELLI, 939, 453, 612, 304, 194, 410, 1, 7, 0))

scala> rdd_origin_split_trim.count()
res2: Long = 8000


scala> val header = rdd_origin_split_trim.filter(arr => arr.contains("DESCREGIONE"))
header: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[4] at filter at <console>:33

scala> header.take(2)
res3: Array[Array[String]] = Array(Array(DESCREGIONE, DESCPROVINCIA, DESCCOMUNE, ELETTORI, ELETTORI_M, VOTANTI, VOTANTI_M, NUMVOTISI, NUMVOTINO, NUMVOTIBIANCHI, NUMVOTINONVALIDI, NUMVOTICONTESTATI))

scala> val rdd_body = rdd_origin_split_trim.filter(arr => !arr.contains("DESCREGIONE"))
rdd_body: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[5] at filter at <console>:33

scala> rdd_body.take(5)
res4: Array[Array[String]] = Array(Array(ABRUZZO, CHIETI, ALTINO, 2288, 1101, 1496, 775, 533, 953, 2, 8, 0), Array(ABRUZZO, CHIETI, ARCHI, 1785, 861, 1241, 632, 442, 782, 3, 14, 0), Array(ABRUZZO, CHIETI, ARI, 831, 402, 617, 328, 241, 366, 6, 4, 0), Array(ABRUZZO, CHIETI, ARIELLI, 939, 453, 612, 304, 194, 410, 1, 7, 0), Array(ABRUZZO, CHIETI, ATESSA, 8454, 4121, 5860, 3006, 1952, 3836, 45, 27, 0))

scala> rdd_body.count()
res5: Long = 7999

scala> rdd_body.filter(arr => arr.contains("")).filter( arr => arr.contains("ROVIGO"))
res12: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[11] at filter at <console>:36

scala> rdd_body.filter(arr => arr.contains("")).filter( arr => arr.contains("ROVIGO")).take(5)
res13: Array[Array[String]] = Array(Array(VENETO, ROVIGO, GAIBA, 840, "", 642, 320, 275, 361, 3, 3, 0))

scala> val rdd_1=rdd_body.filter(arr => !arr.contains("")).filter( arr => !arr.contains("ROVIGO"))
rdd_1: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[15] at filter at <console>:35

scala> rdd_1.filter(arr => arr.contains("")).filter( arr => arr.contains("ROVIGO")).take(5)
res14: Array[Array[String]] = Array()

scala> rdd_1.count()
res15: Long = 7946

scala> rdd_body.count()
res16: Long = 7999

scala> rdd_body.filter(arr => arr.contains(""))
res17: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[18] at filter at <console>:36

scala> rdd_body.filter(arr => arr.contains("")).take(5)
res18: Array[Array[String]] = Array(Array(CALABRIA, COSENZA, "", 4436, 2156, 2783, 1435, 765, 1985, 10, 23, 0), Array(SICILIA, CATANIA, "", 18623, 8816, 12404, 6032, 3084, 9248, 7, 65, 0), Array(VENETO, ROVIGO, GAIBA, 840, "", 642, 320, 275, 361, 3, 3, 0), Array(""))

scala> rdd_body.filter(arr => arr.contains("?"))
res19: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[20] at filter at <console>:36

scala> rdd_body.filter(arr => arr.contains("?")).take(5)
res20: Array[Array[String]] = Array(Array(VENETO, TREVISO, CESSALTO, ?, ?, ?, 973, 712, 1188, 6, 11, 0))

scala> val rdd_1=rdd_body.filter(arr => !arr.contains("?"))
rdd_1: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[22] at filter at <console>:35

scala> rdd_1.count()
res21: Long = 7998

scala> rdd_1.filter(arr => arr.contains("Nan"))
res22: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[23] at filter at <console>:38

scala> rdd_1.filter(arr => arr.contains("Nan")).take(5)
res23: Array[Array[String]] = Array(Array(VENETO, ROVIGO, FRASSINELLE POLESINE, Nan, 585, 899, 467, 327, 560, 7, 5, 0))

scala> val rdd_2=rdd_1.filter(arr => !arr.contains("Nan"))
rdd_2: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[25] at filter at <console>:37

scala> rdd_2.count()
res24: Long = 7997

scala> rdd_2.filter(arr => arr.contains("GAIBA"))
res25: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[26] at filter at <console>:40

scala> rdd_2.filter(arr => arr.contains("GAIBA")).take(3)
res26: Array[Array[String]] = Array(Array(VENETO, ROVIGO, GAIBA, 840, "", 642, 320, 275, 361, 3, 3, 0))

scala> val rdd_3=rdd_2.filter(arr => !arr.contains("GAIBA"))
rdd_3: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[28] at filter at <console>:39

scala> rdd_3.count()
res27: Long = 7996

scala> rdd_3.filter(_.forall(_.isEmpty))
res28: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[29] at filter at <console>:42

scala> rdd_3.filter(_.forall(_.isEmpty)).take(5)
res29: Array[Array[String]] = Array(Array(""))

scala> val rdd_4=rdd_3.filter(_.forall(!_.isEmpty))
rdd_4: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[31] at filter at <console>:41

scala> rdd_4.count()
res30: Long = 7993

scala> rdd_5.filter(arr => arr.contains("#893"))
res34: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[33] at filter at <console>:44

scala> rdd_5.filter(arr => arr.contains("#893")).take(5)
res35: Array[Array[String]] = Array(Array(VENETO, VICENZA, SAN GERMANO DEI BERICI, #893, 454, 742, 384, 218, 513, 1, 10, 0))

scala> rdd_5.filter(arr => arr.map(el => if(el=="#893") "893" else el))
<console>:44: error: type mismatch;
 found   : Array[String]
 required: Boolean
              rdd_5.filter(arr => arr.map(el => if(el=="#893") "893" else el))
                                         ^

scala> rdd_5.map (arr => arr.map (el => if(el=="#893") "893" else el))
res37: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[35] at map at <console>:44

scala> val rdd_6=rdd_5.map (arr => arr.map (el => if(el=="#893") "893" else el))
rdd_6: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[36] at map at <console>:43

scala> rdd_6.filter(arr => arr.contains("#893")).take(5)
res38: Array[Array[String]] = Array()

scala> rdd_6.filter(arr => arr.contains("\3360")).take(5)
res39: Array[Array[String]] = Array()

scala> rdd_6.filter(arr => arr.contains("/3360")).take(5)
res40: Array[Array[String]] = Array(Array(VENETO, VICENZA, SOSSANO, /3360, 1694, -2629, 1350, 842, 1761, 6, 20, 0))

scala> val rdd_7=rdd_6.map (arr => arr.map (el => if(el=="/3360") "3360" else el))
rdd_7: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[40] at map at <console>:45

scala> rdd_7.filter(arr => arr.contains("/3360")).take(5)
res41: Array[Array[String]] = Array()

scala> rdd_7.filter(arr => arr.contains("SICLIA")).take(5)
res42: Array[Array[String]] = Array(Array(SICLIA, CATANIA, RANDAZZO, 8919, 4223, 5173, 2645, 1384, 3733, 16, 40, 0))

scala> val rdd_8=rdd_7.map (arr => arr.map (el => if(el=="SICLIA") "SICILIA" else el))
rdd_8: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[43] at map at <console>:47

scala> val rdd_9=rdd_8.filter(arr => !arr.contains("POLINO"))
rdd_9: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[44] at filter at <console>:49

scala> val rdd_10=rdd_9.filter(arr => !arr.contains("SOSSANO"))
rdd_10: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[45] at filter at <console>:51

scala> rdd_10.count()
res43: Long = 7993

scala> val cleaned = rdd_10;
rdd_final: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[45] at filter at <console>:51

scala> rdd_cleaned.count()
res44: Long = 7993

val rdd_regioni=rdd_cleaned.map(arr => (arr(0),(arr(0),arr(3).toInt,arr(4).toInt,arr(3).toInt-arr(4).toInt,arr(5).toInt,arr(6).toInt,arr(7).toInt,arr(8).toInt,arr(9).toInt,arr(10).toInt,arr(11).toInt)))
rdd_regioni: org.apache.spark.rdd.RDD[(String, (String, Int, Int, Int, Int, Int, Int, Int, Int, Int, Int))] = MapPartitionsRDD[35] at map at <console>:51

scala> rdd_regioni.count()
res40: Long = 7991

scala> rdd_regioni.take(3)
res41: Array[(String, (String, Int, Int, Int, Int, Int, Int, Int, Int, Int, Int))] = Array((ABRUZZO,(ABRUZZO,2288,1101,1187,1496,775,533,953,2,8,0)), (ABRUZZO,(ABRUZZO,1785,861,924,1241,632,442,782,3,14,0)), (ABRUZZO,(ABRUZZO,831,402,429,617,328,241,366,6,4,0)))

rdd_regioni.reduceByKey((a,b)=>(a._1+b._1,a._2+b._2,a._3+b._3,a._4+b._4,a._5+b._5,a._6+b._6,a._7+b._7,a._8+b._8,a._9+b._9,a._10+b._10,a._11+b._11))
res60: org.apache.spark.rdd.RDD[(String, (String, Int, Int, Int, Int, Int, Int, Int, Int, Int, Int))] = ShuffledRDD[36] at reduceByKey at <console>:54

scala> rdd_regioni.take(5)
res61: Array[(String, (String, Int, Int, Int, Int, Int, Int, Int, Int, Int, Int))] = Array((ABRUZZO,(ABRUZZO,2288,1101,1187,1496,775,533,953,2,8,0)), (ABRUZZO,(ABRUZZO,1785,861,924,1241,632,442,782,3,14,0)), (ABRUZZO,(ABRUZZO,831,402,429,617,328,241,366,6,4,0)), (ABRUZZO,(ABRUZZO,939,453,486,612,304,194,410,1,7,0)), (ABRUZZO,(ABRUZZO,8454,4121,4333,5860,3006,1952,3836,45,27,0)))

scala> rdd_regioni.reduceByKey((a,b)=>(a._1+b._1,a._2+b._2,a._3+b._3,a._4+b._4,a._5+b._5,a._6+b._6,a._7+b._7,a._8+b._8,a._9+b._9,a._10+b._10,a._11+b._11)).take(5)
res62: Array[(String, (String, Int, Int, Int, Int, Int, Int, Int, Int, Int, Int))] = Array((ABRUZZO,(ABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABRUZZOABR...
scala> rdd_regioni.reduceByKey((a,b)=>(a._1,a._2+b._2,a._3+b._3,a._4+b._4,a._5+b._5,a._6+b._6,a._7+b._7,a._8+b._8,a._9+b._9,a._10+b._10,a._11+b._11)).take(5)
res63: Array[(String, (String, Int, Int, Int, Int, Int, Int, Int, Int, Int, Int))] = Array((ABRUZZO,(ABRUZZO,1052049,509116,542933,722930,362259,255001,461188,2117,4611,13)), (TRENTINO-ALTO ADIGE,(TRENTINO-ALTO ADIGE,792504,387839,404665,572486,282999,305322,261473,2698,2985,8)), (MARCHE,(MARCHE,1189181,574341,614840,866233,434942,385768,472765,2609,5074,17)), (SICILIA,(SICILIA,4013248,1915609,2097639,2271850,1143565,639629,1610847,4451,16822,101)), (LAZIO,(LAZIO,4402145,2097786,2304359,3044673,1486411,1108768,1914397,4678,16750,80)))

scala> rdd_regioni.reduceByKey((a,b)=>(a._1,a._2+b._2,a._3+b._3,a._4+b._4,a._5+b._5,a._6+b._6,a._7+b._7,a._8+b._8,a._9+b._9,a._10+b._10,a._11+b._11)).count()
res64: Long = 20

scala> val rdd_regioni_1=rdd_regioni.reduceByKey((a,b)=>(a._1,a._2+b._2,a._3+b._3,a._4+b._4,a._5+b._5,a._6+b._6,a._7+b._7,a._8+b._8,a._9+b._9,a._10+b._10,a._11+b._11))
rdd_regioni_1: org.apache.spark.rdd.RDD[(String, (String, Int, Int, Int, Int, Int, Int, Int, Int, Int, Int))] = ShuffledRDD[40] at reduceByKey at <console>:53

scala> rdd_regioni_1.take(2)
res65: Array[(String, (String, Int, Int, Int, Int, Int, Int, Int, Int, Int, Int))] = Array((ABRUZZO,(ABRUZZO,1052049,509116,542933,722930,362259,255001,461188,2117,4611,13)), (TRENTINO-ALTO ADIGE,(TRENTINO-ALTO ADIGE,792504,387839,404665,572486,282999,305322,261473,2698,2985,8)))

scala> val rdd_final = rdd_regioni_1.map( arr => arr._1)
rdd_final: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[41] at map at <console>:55

scala> rdd_final.take(2)
res66: Array[String] = Array(ABRUZZO, TRENTINO-ALTO ADIGE)

scala> rdd_origin_split.take(5)
res71: Array[Array[String]] = Array(Array(DESCREGIONE, DESCPROVINCIA, DESCCOMUNE, ELETTORI, ELETTORI_M, VOTANTI, VOTANTI_M, NUMVOTISI, NUMVOTINO, NUMVOTIBIANCHI, NUMVOTINONVALIDI, NUMVOTICONTESTATI), Array(ABRUZZO, "CHIETI                        ", "ALTINO                                                                                              ", 2288, 1101, 1496, 775, 533, 953, 2, 8, 0), Array(ABRUZZO, "CHIETI                        ", "ARCHI                                                                                               ", 1785, 861, 1241, 632, 442, 782, 3, 14, 0), Array(ABRUZZO, "CHIETI                        ", "ARI                                                                                                 ", 831, 402, 617, 328, 241, 366, 6, 4, 0), Array(ABRUZZ...
scala> rdd_regioni_1.take(1)
res72: Array[(String, (String, Int, Int, Int, Int, Int, Int, Int, Int, Int, Int))] = Array((ABRUZZO,(ABRUZZO,1052049,509116,542933,722930,362259,255001,461188,2117,4611,13)))

scala> val rdd_final = rdd_regioni_1.map(arr => (arr._1,arr._3,arr._4))
<console>:55: error: value _3 is not a member of (String, (String, Int, Int, Int, Int, Int, Int, Int, Int, Int, Int))
         val rdd_final = rdd_regioni_1.map(arr => (arr._1,arr._3,arr._4))
                                                              ^
<console>:55: error: value _4 is not a member of (String, (String, Int, Int, Int, Int, Int, Int, Int, Int, Int, Int))
         val rdd_final = rdd_regioni_1.map(arr => (arr._1,arr._3,arr._4))
                                                                     ^

scala> val rdd_final = rdd_regioni_1.map(arr => (arr._1,arr._2))
rdd_final: org.apache.spark.rdd.RDD[(String, (String, Int, Int, Int, Int, Int, Int, Int, Int, Int, Int))] = MapPartitionsRDD[42] at map at <console>:55

scala> rdd_final.take(1)
res73: Array[(String, (String, Int, Int, Int, Int, Int, Int, Int, Int, Int, Int))] = Array((ABRUZZO,(ABRUZZO,1052049,509116,542933,722930,362259,255001,461188,2117,4611,13)))

scala> val rdd_final = rdd_regioni_1.map(arr => (arr._1,arr._2(1)))
<console>:55: error: (String, Int, Int, Int, Int, Int, Int, Int, Int, Int, Int) does not take parameters
         val rdd_final = rdd_regioni_1.map(arr => (arr._1,arr._2(1)))
                                                                ^

scala> val rdd_final = rdd_regioni_1.map(arr => (arr._1,arr._2._1))
rdd_final: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[43] at map at <console>:55

scala> rdd_final.take(1)
res74: Array[(String, String)] = Array((ABRUZZO,ABRUZZO))

scala> rdd_regioni_1.map(arr => (arr._1,arr._2._2,arr._2._3)).take(1)
res75: Array[(String, Int, Int)] = Array((ABRUZZO,1052049,509116))

scala> rdd_regioni_1.map(arr => (arr._1,arr._2._2,arr._2._3,arr._2._4)).take(1)
res76: Array[(String, Int, Int, Int)] = Array((ABRUZZO,1052049,509116,542933))

scala> rdd_regioni_1.map(arr => (arr._1,arr._2._2,arr._2._3,arr._2._4,arr._2._5/arr._2._2)).take(1)
res77: Array[(String, Int, Int, Int, Int)] = Array((ABRUZZO,1052049,509116,542933,0))

scala> rdd_regioni_1.map(arr => (arr._1,arr._2._2,arr._2._3,arr._2._4,arr._2._5)).take(1)
res78: Array[(String, Int, Int, Int, Int)] = Array((ABRUZZO,1052049,509116,542933,722930))

scala> rdd_regioni_1.map(arr => (arr._1,arr._2._2,arr._2._3,arr._2._4,arr._2._5.toFloat/arr._2._2.toFloat*100)).take(1)
res79: Array[(String, Int, Int, Int, Float)] = Array((ABRUZZO,1052049,509116,542933,68.716385))

scala> rdd_regioni_1.map(arr => (arr._1,arr._2._2,arr._2._3,arr._2._4,arr._2._5.toFloat/arr._2._2.toFloat*100,arr._2._6.toFloat/arr._2._2.toFloat*100)).take(1)
res80: Array[(String, Int, Int, Int, Float, Float)] = Array((ABRUZZO,1052049,509116,542933,68.716385,34.433662))

scala> rdd_regioni_1.map(arr => (arr._1,arr._2._2,arr._2._3,arr._2._4,arr._2._5.toFloat/arr._2._2.toFloat*100,arr._2._6.toFloat/arr._2._2.toFloat*100)).take(2)
res81: Array[(String, Int, Int, Int, Float, Float)] = Array((ABRUZZO,1052049,509116,542933,68.716385,34.433662), (TRENTINO-ALTO ADIGE,792504,387839,404665,72.23762,35.709473))

scala> rdd_regioni_1.map(arr => (arr._1,arr._2._2,arr._2._3,arr._2._4,arr._2._5.toFloat/arr._2._2.toFloat*100,arr._2._6.toFloat/arr._2._5.toFloat*100)).take(2)
res82: Array[(String, Int, Int, Int, Float, Float)] = Array((ABRUZZO,1052049,509116,542933,68.716385,50.109833), (TRENTINO-ALTO ADIGE,792504,387839,404665,72.23762,49.433346))

scala> rdd_regioni_1.map(arr => (arr._1,arr._2._2,arr._2._3,arr._2._4,arr._2._5.toFloat/arr._2._2.toFloat*100)).take(2)
res83: Array[(String, Int, Int, Int, Float)] = Array((ABRUZZO,1052049,509116,542933,68.716385), (TRENTINO-ALTO ADIGE,792504,387839,404665,72.23762))

scala> rdd_regioni_1.map(arr => (arr._1,arr._2._2,arr._2._3,arr._2._4,arr._2._5.toFloat/arr._2._2.toFloat*100,arr._2._7.toFloat/arr._2._5.toFloat*100)).take(2)
res84: Array[(String, Int, Int, Int, Float, Float)] = Array((ABRUZZO,1052049,509116,542933,68.716385,35.273262), (TRENTINO-ALTO ADIGE,792504,387839,404665,72.23762,53.332657))

scala> rdd_regioni_1.map(arr => (arr._1,arr._2._2,arr._2._3,arr._2._4,arr._2._5.toFloat/arr._2._2.toFloat*100,arr._2._7.toFloat/arr._2._5.toFloat*100,arr._2._8.toFloat/arr._2._5.toFloat*100)).take(2)
res85: Array[(String, Int, Int, Int, Float, Float, Float)] = Array((ABRUZZO,1052049,509116,542933,68.716385,35.273262,63.79428), (TRENTINO-ALTO ADIGE,792504,387839,404665,72.23762,53.332657,45.673256))

scala> rdd_regioni_1.map(arr => (arr._1,arr._2._2,arr._2._3,arr._2._4,arr._2._5.toFloat/arr._2._2.toFloat*100,arr._2._7.toFloat/arr._2._5.toFloat*100,arr._2._8.toFloat/arr._2._5.toFloat*100,(arr._2._9.toFloat+arr._2._10.toFloat+arr._2._10)/arr._2._5.toFloat*100)).take(2)
res86: Array[(String, Int, Int, Int, Float, Float, Float, Float)] = Array((ABRUZZO,1052049,509116,542933,68.716385,35.273262,63.79428,1.5684783), (TRENTINO-ALTO ADIGE,792504,387839,404665,72.23762,53.332657,45.673256,1.5140982))

scala> rdd_regioni.take(2)
res87: Array[(String, (String, Int, Int, Int, Int, Int, Int, Int, Int, Int, Int))] = Array((ABRUZZO,(ABRUZZO,2288,1101,1187,1496,775,533,953,2,8,0)), (ABRUZZO,(ABRUZZO,1785,861,924,1241,632,442,782,3,14,0)))

scala> rdd_regioni_1.map(arr => (arr._1,arr._2._2,arr._2._3,arr._2._4,arr._2._5.toFloat/arr._2._2.toFloat*100,arr._2._7.toFloat/arr._2._5.toFloat*100,arr._2._8.toFloat/arr._2._5.toFloat*100,(arr._2._9.toFloat+arr._2._10.toFloat+arr._2._11.toFloat)/arr._2._5.toFloat*100)).take(2)
res88: Array[(String, Int, Int, Int, Float, Float, Float, Float)] = Array((ABRUZZO,1052049,509116,542933,68.716385,35.273262,63.79428,0.9324554), (TRENTINO-ALTO ADIGE,792504,387839,404665,72.23762,53.332657,45.673256,0.99408543))

scala> val rdd_final=rdd_regioni_1.map(arr => (arr._1,arr._2._2,arr._2._3,arr._2._4,arr._2._5.toFloat/arr._2._2.toFloat*100,arr._2._7.toFloat/arr._2._5.toFloat*100,arr._2._8.toFloat/arr._2._5.toFloat*100,(arr._2._9.toFloat+arr._2._10.toFloat+arr._2._10)/arr._2._5.toFloat*100))
rdd_final: org.apache.spark.rdd.RDD[(String, Int, Int, Int, Float, Float, Float, Float)] = MapPartitionsRDD[57] at map at <console>:55

scala> rdd_final.take(20)
res89: Array[(String, Int, Int, Int, Float, Float, Float, Float)] = Array((ABRUZZO,1052049,509116,542933,68.716385,35.273262,63.79428,1.5684783), (TRENTINO-ALTO ADIGE,792504,387839,404665,72.23762,53.332657,45.673256,1.5140982), (MARCHE,1189181,574341,614840,72.84283,44.533978,54.577118,1.4726985), (SICILIA,4013248,1915609,2097639,56.60876,28.154543,70.90463,1.6768273), (LAZIO,4402145,2097786,2304359,69.16339,36.416653,62.876934,1.2539278), (PIEMONTE,3396378,1633247,1763131,72.037445,43.10968,55.945606,1.6306692), (CALABRIA,1549305,747248,802057,54.410976,32.675163,66.39932,1.5860174), (LOMBARDIA,7480375,3616467,3863908,74.2277,44.177067,55.077976,1.2546937), (FRIULI-VENEZIA GIULIA,952494,457538,494956,72.51668,38.70717,60.48121,1.3736451), (MOLISE,256600,124778,131822,63.927513,38.8294...

scala>rdd_final.saveAsTextFile("referendum2016-aggregated.csv") 





